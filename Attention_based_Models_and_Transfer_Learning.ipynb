{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention based Models and Transfer Learning"
      ],
      "metadata": {
        "id": "QkEJIgwjq0jv"
      },
      "id": "QkEJIgwjq0jv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2911e8a0",
      "metadata": {
        "id": "2911e8a0"
      },
      "outputs": [],
      "source": [
        "# Q1. What is BERT and how does it work?\n",
        "\n",
        "# answer\n",
        "# BERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model developed by Google.\n",
        "# It learns context bidirectionally by attending to both left and right tokens simultaneously.\n",
        "# BERT is pretrained using Masked Language Modeling (MLM) and Next Sentence Prediction (NSP),\n",
        "# and later fine-tuned on specific NLP tasks such as classification and question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13986424",
      "metadata": {
        "id": "13986424"
      },
      "outputs": [],
      "source": [
        "# Q2. What are the main advantages of using the attention mechanism in neural networks?\n",
        "\n",
        "# answer\n",
        "# The attention mechanism allows models to focus on relevant parts of the input sequence,\n",
        "# handle long-range dependencies, improve interpretability, and reduce the limitations of fixed-size context windows\n",
        "# found in RNNs and CNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76089fab",
      "metadata": {
        "id": "76089fab"
      },
      "outputs": [],
      "source": [
        "# Q3. How does the self-attention mechanism differ from traditional attention mechanisms?\n",
        "\n",
        "# answer\n",
        "# Traditional attention attends to encoder states from a decoder, while self-attention allows tokens in the same\n",
        "# sequence to attend to each other. This enables parallelization and better modeling of dependencies across the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5739ca66",
      "metadata": {
        "id": "5739ca66"
      },
      "outputs": [],
      "source": [
        "# Q4. What is the role of the decoder in a Seq2Seq model?\n",
        "\n",
        "# answer\n",
        "# The decoder generates the output sequence step by step, conditioned on the encoder’s context vector\n",
        "# and its previously generated tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c441449",
      "metadata": {
        "id": "9c441449"
      },
      "outputs": [],
      "source": [
        "# Q5. What is the difference between GPT-2 and BERT models?\n",
        "\n",
        "# answer\n",
        "# BERT is bidirectional and designed mainly for understanding tasks, while GPT-2 is unidirectional (autoregressive)\n",
        "# and designed for text generation. BERT uses MLM and NSP during pretraining, GPT-2 predicts the next word\n",
        "# in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e754d7b1",
      "metadata": {
        "id": "e754d7b1"
      },
      "outputs": [],
      "source": [
        "# Q6. Why is the Transformer model considered more efficient than RNNs and LSTMs?\n",
        "\n",
        "# answer\n",
        "# Transformers eliminate sequential dependencies by using self-attention, allowing parallelization and\n",
        "# capturing long-range dependencies without vanishing gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da2a327b",
      "metadata": {
        "id": "da2a327b"
      },
      "outputs": [],
      "source": [
        "# Q7. Explain how the attention mechanism works in a Transformer model.\n",
        "\n",
        "# answer\n",
        "# The attention mechanism computes weighted combinations of values (V), where weights are obtained by\n",
        "# comparing queries (Q) with keys (K). Scaled dot-product attention is applied to determine relevance,\n",
        "# allowing the model to focus on important tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e14811c",
      "metadata": {
        "id": "0e14811c"
      },
      "outputs": [],
      "source": [
        "# Q8. What is the difference between an encoder and a decoder in a Seq2Seq model?\n",
        "\n",
        "# answer\n",
        "# The encoder processes the input sequence into context representations, while the decoder generates the\n",
        "# output sequence step by step using the encoder’s output and its own previous outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41dbcee",
      "metadata": {
        "id": "b41dbcee"
      },
      "outputs": [],
      "source": [
        "# Q9. What is the primary purpose of using the self-attention mechanism in transformers?\n",
        "\n",
        "# answer\n",
        "# Self-attention allows each token to capture contextual relationships with every other token in the sequence,\n",
        "# enabling better handling of semantic dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "172ec7b9",
      "metadata": {
        "id": "172ec7b9"
      },
      "outputs": [],
      "source": [
        "# Q10. How does the GPT-2 model generate text?\n",
        "\n",
        "# answer\n",
        "# GPT-2 generates text autoregressively by predicting the next token in a sequence using previously generated tokens\n",
        "# and the transformer decoder architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078309e7",
      "metadata": {
        "id": "078309e7"
      },
      "outputs": [],
      "source": [
        "# Q11. What is the main difference between the encoder-decoder architecture and a simple neural network?\n",
        "\n",
        "# answer\n",
        "# Encoder-decoder architectures are specifically designed for sequence-to-sequence tasks like translation,\n",
        "# while simple neural networks lack the ability to encode contextual sequence dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dfad2f9",
      "metadata": {
        "id": "0dfad2f9"
      },
      "outputs": [],
      "source": [
        "# Q12. Explain the concept of “fine-tuning” in BERT.\n",
        "\n",
        "# answer\n",
        "# Fine-tuning involves taking a pretrained BERT model and adapting it to a specific downstream task\n",
        "# (e.g., sentiment classification, NER) by training it on task-specific labeled data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bfba08d",
      "metadata": {
        "id": "4bfba08d"
      },
      "outputs": [],
      "source": [
        "# Q13. How does the attention mechanism handle long-range dependencies in sequences?\n",
        "\n",
        "# answer\n",
        "# Attention directly computes pairwise relationships between tokens, regardless of distance,\n",
        "# avoiding the vanishing gradient problem of RNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c649dd3",
      "metadata": {
        "id": "7c649dd3"
      },
      "outputs": [],
      "source": [
        "# Q14. What is the core principle behind the Transformer architecture?\n",
        "\n",
        "# answer\n",
        "# The core principle is self-attention, which enables parallel computation and effective modeling\n",
        "# of dependencies without recurrence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae1d604",
      "metadata": {
        "id": "eae1d604"
      },
      "outputs": [],
      "source": [
        "# Q15. What is the role of the \"position encoding\" in a Transformer model?\n",
        "\n",
        "# answer\n",
        "# Since Transformers lack recurrence, position encodings are added to input embeddings to provide information\n",
        "# about the order of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92a8f8d",
      "metadata": {
        "id": "f92a8f8d"
      },
      "outputs": [],
      "source": [
        "# Q16. How do Transformers use multiple layers of attention?\n",
        "\n",
        "# answer\n",
        "# Transformers stack multiple attention layers (multi-head attention) to capture different types of relationships\n",
        "# in the sequence simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0fbd1dd",
      "metadata": {
        "id": "d0fbd1dd"
      },
      "outputs": [],
      "source": [
        "# Q17. What does it mean when a model is described as “autoregressive” like GPT-2?\n",
        "\n",
        "# answer\n",
        "# Autoregressive means the model generates one token at a time, conditioned on previously generated tokens,\n",
        "# predicting the next word step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea17ba6a",
      "metadata": {
        "id": "ea17ba6a"
      },
      "outputs": [],
      "source": [
        "# Q18. How does BERT's bidirectional training improve its performance?\n",
        "\n",
        "# answer\n",
        "# Bidirectional training allows BERT to learn context from both left and right tokens,\n",
        "# leading to richer contextual representations compared to unidirectional models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454f02f8",
      "metadata": {
        "id": "454f02f8"
      },
      "outputs": [],
      "source": [
        "# Q19. What are the advantages of using the Transformer over RNN-based models in NLP?\n",
        "\n",
        "# answer\n",
        "# Transformers offer parallelization, better handling of long-range dependencies,\n",
        "# scalability, and superior performance on NLP benchmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f622aa9",
      "metadata": {
        "id": "8f622aa9"
      },
      "outputs": [],
      "source": [
        "# Q20. What is the attention mechanism’s impact on the performance of models like BERT and GPT-2?\n",
        "\n",
        "# answer\n",
        "# The attention mechanism improves context understanding, enables handling of long sequences,\n",
        "# and significantly boosts the performance of language understanding and generation tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e202dd8",
      "metadata": {
        "id": "4e202dd8"
      },
      "source": [
        "# Practical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b76079",
      "metadata": {
        "id": "46b76079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3199ff20-5698-4f4f-a5b7-d7d9e365adde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.4000 - loss: 0.6946\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.4000 - loss: 0.6934\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6000 - loss: 0.6921\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6000 - loss: 0.6909\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6000 - loss: 0.6897\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6000 - loss: 0.6885\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.6000 - loss: 0.6873\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6000 - loss: 0.6861\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.6000 - loss: 0.6848\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.6000 - loss: 0.6836\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step\n",
            "Test Sentences: ['I enjoy learning NLP', 'I hate this subject']\n",
            "Predictions (probabilities): [[0.5223585 ]\n",
            " [0.51489764]]\n",
            "Predicted Labels: [1 1]\n"
          ]
        }
      ],
      "source": [
        "# Q1. How to implement a simple text classification model using LSTM in Keras?\n",
        "\n",
        "#code >\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Sample dataset (tiny dataset, better accuracy with more data)\n",
        "sentences = [\n",
        "    \"I love this course\",       # Positive\n",
        "    \"This is an amazing class\", # Positive\n",
        "    \"I enjoy learning NLP\",     # Positive\n",
        "    \"I hate this subject\",      # Negative\n",
        "    \"This class is boring\"      # Negative\n",
        "]\n",
        "\n",
        "labels = [1, 1, 1, 0, 0]  # 1=Positive, 0=Negative\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Padding sequences to same length\n",
        "X = pad_sequences(sequences, padding='post')\n",
        "y = np.array(labels)\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=16))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=10, verbose=1)\n",
        "\n",
        "# Test on new data\n",
        "test_sentences = [\"I enjoy learning NLP\", \"I hate this subject\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_pad = pad_sequences(test_seq, maxlen=X.shape[1], padding='post')\n",
        "\n",
        "predictions = model.predict(test_pad)\n",
        "predicted_labels = (predictions > 0.5).astype(int)\n",
        "\n",
        "#example >\n",
        "print(\"Test Sentences:\", test_sentences)\n",
        "print(\"Predictions (probabilities):\", predictions)\n",
        "print(\"Predicted Labels:\", predicted_labels.reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bbffce9",
      "metadata": {
        "id": "6bbffce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b0de1a-d807-4b56-85e3-cfa6d9282635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: I love learning nlp with deep learning\n"
          ]
        }
      ],
      "source": [
        "# Q2. How to generate sequences of text using a Recurrent Neural Network (RNN)?\n",
        "\n",
        "#code >\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "# Sample corpus\n",
        "data = \"\"\"I love learning NLP with deep learning.\n",
        "RNN models can generate sequences of text.\n",
        "This is a simple RNN text generation example.\"\"\"\n",
        "\n",
        "# Tokenize text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text into sequences\n",
        "input_sequences = []\n",
        "for line in data.split(\".\"):\n",
        "    tokens = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(tokens)):\n",
        "        n_gram = tokens[:i+1]\n",
        "        input_sequences.append(n_gram)\n",
        "\n",
        "# Pad sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "max_seq_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Define RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 16, input_length=max_seq_len-1))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"I love\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            seed_text += \" \" + word\n",
        "            break\n",
        "\n",
        "#example >\n",
        "print(\"Generated Text:\", seed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f30165b",
      "metadata": {
        "id": "8f30165b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "adbd7e6f-7624-4ab0-af13-d05c0876c462"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# Q3. How to perform sentiment analysis using a simple CNN model?\n",
        "\n",
        "#code >\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "\n",
        "def cnn_sentiment_classifier(vocab_size=5000, embedding_dim=64, input_length=100):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=input_length),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "#example\n",
        "cnn_model = cnn_sentiment_classifier()\n",
        "print(cnn_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e3da85d",
      "metadata": {
        "id": "1e3da85d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d148616a-ee5a-4a28-b9a0-3971bedd50bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
          ]
        }
      ],
      "source": [
        "# Q4. How to perform Named Entity Recognition (NER) using spaCy?\n",
        "\n",
        "#code >\n",
        "import spacy\n",
        "\n",
        "def perform_ner(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "#example\n",
        "print(perform_ner(\"Apple is looking at buying U.K. startup for $1 billion\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c935891c",
      "metadata": {
        "id": "c935891c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "d11abbf9-41fc-4fe5-baca-750452ad9ea1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m365,568\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m365,568\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │     \u001b[38;5;34m25,700\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,700</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m756,836\u001b[0m (2.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">756,836</span> (2.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m756,836\u001b[0m (2.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">756,836</span> (2.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# Q5. How to implement a simple Seq2Seq model for machine translation using LSTM in Keras?\n",
        "\n",
        "#code >\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "def seq2seq_model(latent_dim=256, input_dim=100, output_dim=100):\n",
        "    encoder_inputs = Input(shape=(None, input_dim))\n",
        "    encoder = LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(None, output_dim))\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(output_dim, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "    return model\n",
        "\n",
        "#example\n",
        "seq2seq = seq2seq_model()\n",
        "print(seq2seq.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e9cda3",
      "metadata": {
        "id": "b8e9cda3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9400955e-f1e3-4fcf-e466-10761f6a08df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Natural Language Processing with transformers is\n",
            "Generated Text: Natural Language Processing with transformers is a tool that allows you to generate code that will run on your computer. It has been developed by the team at Intel and is used by the IBM Watson.\n",
            "\n",
            "This is a step-by-step guide to making your own transformers. It will help you to make the most of your existing data sets and make the most of your existing data sets, without having to build your own transform\n"
          ]
        }
      ],
      "source": [
        "# Q6. How to generate text using a pre-trained transformer model (GPT-2)?\n",
        "\n",
        "#code >\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load tokenizer & model\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# GPT-2 has no native pad token; set it to eos to avoid warnings\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Prompt\n",
        "prompt = \"Natural Language Processing with transformers is\"\n",
        "\n",
        "# Tokenize WITH attention_mask and padding\n",
        "enc = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Generate (note: using max_new_tokens; and passing attention_mask & pad_token_id)\n",
        "out_ids = model.generate(\n",
        "    input_ids=enc[\"input_ids\"],\n",
        "    attention_mask=enc[\"attention_mask\"],\n",
        "    max_new_tokens=80,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "#example >\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"Generated Text:\", tokenizer.decode(out_ids[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5997c53",
      "metadata": {
        "id": "d5997c53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd640a4-1abc-4e6a-806b-5c8f2d83d5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am cheerful but sometimes unhappy\n"
          ]
        }
      ],
      "source": [
        "# Q7. How to apply data augmentation for text in NLP?\n",
        "#code >\n",
        "import random\n",
        "\n",
        "def synonym_replacement(text, synonyms={\"happy\":[\"joyful\",\"cheerful\"],\"sad\":[\"unhappy\",\"sorrowful\"]}):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word in synonyms:\n",
        "            new_words.append(random.choice(synonyms[word]))\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "#example\n",
        "print(synonym_replacement(\"I am happy but sometimes sad\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f02bbe0d",
      "metadata": {
        "id": "f02bbe0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "98daa879-5741-43bd-fde4-fac3e05547cd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m365,568\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m365,568\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │     \u001b[38;5;34m25,700\u001b[0m │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,700</span> │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m756,836\u001b[0m (2.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">756,836</span> (2.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m756,836\u001b[0m (2.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">756,836</span> (2.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# Q8. How can you add an Attention Mechanism to a Seq2Seq model?\n",
        "\n",
        "#code >\n",
        "from tensorflow.keras.layers import Attention\n",
        "\n",
        "def add_attention_to_seq2seq(latent_dim=256, input_dim=100, output_dim=100):\n",
        "    encoder_inputs = Input(shape=(None, input_dim))\n",
        "    encoder_outputs, state_h, state_c = LSTM(latent_dim, return_sequences=True, return_state=True)(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(None, output_dim))\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "    attn_layer = Attention()\n",
        "    attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
        "    decoder_dense = Dense(output_dim, activation=\"softmax\")\n",
        "    decoder_outputs = decoder_dense(attn_out)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    return model\n",
        "\n",
        "#example\n",
        "attention_seq2seq = add_attention_to_seq2seq()\n",
        "print(attention_seq2seq.summary())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}