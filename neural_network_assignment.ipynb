{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3777f40-58f8-4b54-8b8f-71d8fcb9ebd3",
   "metadata": {},
   "source": [
    "                    Neural Network A Simple Perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f78a31b-e984-4d73-8569-f2e6e6b967f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.What is deep learning, and how is it connected to artificial intelligence?\n",
    "\n",
    "Deep learning is a subset of machine learning, which itself is a subset of artificial intelligence. \n",
    "It involves training artificial neural networks to learn patterns from large amounts of data. Deep \n",
    "learning is particularly effective for tasks like image recognition, natural language processing, and\n",
    "speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744a7df-5ebb-4b7b-823b-3fa5bac38f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.What is a neural network, and what are the different types of neural networks?\n",
    "\n",
    "A neural network is a computational model inspired by the human brain, consisting of layers of\n",
    "interconnected nodes (neurons). Types of neural networks include:\n",
    "Feedforward Neural Networks (FNN)\n",
    "Convolutional Neural Networks (CNN)\n",
    "Recurrent Neural Networks (RNN)\n",
    "Long Short-Term Memory Networks (LSTM)\n",
    "Generative Adversarial Networks (GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab02eb7-a380-4c6f-a10c-cc419b96ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.What is the mathematical structure of a neural network?\n",
    "\n",
    "A neural network consists of layers of neurons, where each neuron computes a weighted sum of its\n",
    "inputs, adds a bias, and applies an activation function. Mathematically:\n",
    "ùë¶=ùëì(ùëäùë•+ùëè)\n",
    "y=f(Wx+b)\n",
    "Here, ùëä represents weights, ùë• the input, ùëè the bias, and ùëì the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8df6ce-4ab5-4efe-927d-a3af7c2c88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.What is an activation function, and why is it essential in neural networks?\n",
    "\n",
    "Activation functions introduce non-linearity into the network, enabling it to learn and \n",
    "represent complex patterns. Without them, the network would behave as a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae2c31-f549-460d-96f9-e33c51191f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.Could you list some common activation functions used in neural networks?\n",
    "\n",
    "Sigmoid\n",
    "ReLU (Rectified Linear Unit)\n",
    "Tanh (Hyperbolic Tangent)\n",
    "Leaky ReLU\n",
    "Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061b564-7c8a-4712-b94e-953a1c94b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.What is a multilayer neural network?\n",
    "\n",
    "A multilayer neural network consists of multiple layers of neurons, including input, hidden, and\n",
    "output layers. It can learn hierarchical representations of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e601389-bccd-47c4-b625-8c64561f1fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.What is a loss function, and why is it crucial for neural network training?\n",
    "\n",
    "A loss function measures the error between the predicted output and the actual target.\n",
    "It guides the optimization process by providing feedback for weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b686afb-de2d-4f3f-b6ab-b98df0f56a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.What are some common types of loss functions?\n",
    "\n",
    "Mean Squared Error (MSE)\n",
    "Cross-Entropy Loss\n",
    "Mean Absolute Error (MAE)\n",
    "Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d6916-1b80-4517-8402-fe6ea9015415",
   "metadata": {},
   "outputs": [],
   "source": [
    "9.How does a neural network learn?\n",
    "\n",
    "Neural networks learn by adjusting weights through backpropagation. This process minimizes \n",
    "the loss by using optimization algorithms like gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa973e-7a33-4ab2-80fd-eb0ea929b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "10.What is an optimizer in neural networks, and why is it necessary?\n",
    "\n",
    "An optimizer adjusts the network's weights to minimize the loss function during training. \n",
    "It ensures faster and more efficient convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced3b9b-90da-4821-a13d-5a23357322f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "11.Could you briefly describe some common optimizers?\n",
    "\n",
    "Stochastic Gradient Descent (SGD)\n",
    "Adam\n",
    "RMSprop\n",
    "Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac577498-de0d-46ed-bc3f-2c9065d87639",
   "metadata": {},
   "outputs": [],
   "source": [
    "12.Can you explain forward and backward propagation in a neural network?\n",
    "\n",
    "Forward propagation: Input data flows through the network to produce output.\n",
    "Backward propagation: Gradients of the loss function are computed and propagated back to\n",
    "update weights using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4171b0-9c67-4a78-a5d7-3abedf1f5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "13.What is weight initialization, and how does it impact training?\n",
    "\n",
    "Weight initialization assigns initial values to weights before training.\n",
    "Proper initialization (e.g., Xavier or He initialization) ensures faster\n",
    "convergence and avoids vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fa79f-2174-4cdb-9e36-8dd8a59a13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "14.What is the vanishing gradient problem in deep learning?\n",
    "\n",
    "In deep networks, gradients can become very small as they propagate backward, \n",
    "making it difficult for weights in earlier layers to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec181a-ab30-4a98-8cec-d2fe7cd644fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "15.What is the exploding gradient problem?\n",
    "\n",
    "This occurs when gradients grow exponentially large during backpropagation,\n",
    "causing unstable updates and divergence in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dbb968-e190-4960-afae-dd9b37e5dfd1",
   "metadata": {},
   "source": [
    "                                       Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ee508-f737-4deb-83e0-e201f1b338e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "How do you create a simple perceptron for basic binary classification?\n",
    "A simple perceptron can be created using libraries like TensorFlow or Keras. Here's an example in Python:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a simple perceptron\n",
    "model = Sequential([\n",
    "    Dense(1, input_dim=2, activation='sigmoid')  # 1 output, 2 inputs\n",
    "])\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25016f-d4bd-4d61-aa80-f6300066a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.How can you build a neural network with one hidden layer using Keras?\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, input_dim=2, activation='relu'),  # Hidden layer with 16 neurons\n",
    "    Dense(1, activation='sigmoid')             # Output layer\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ab2f1-f96f-47c3-84b9-8217f4d4e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.How do you initialize weights using the Xavier (Glorot) initialization method in Keras?\n",
    "\n",
    "In Keras, you can specify Xavier initialization (called \"glorot_uniform\") for the Dense layer:\n",
    "\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, input_dim=2, activation='relu', kernel_initializer=glorot_uniform())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346eded-e576-4898-90a6-b8408f2f27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.How can you apply different activation functions in a neural network in Keras?\n",
    "\n",
    "Activation functions are applied by specifying the activation parameter in each layer:\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu'),    # ReLU activation\n",
    "    Dense(8, activation='tanh'),     # Tanh activation\n",
    "    Dense(1, activation='sigmoid')   # Sigmoid activation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1574e3b-1773-457d-bf68-cdfbb50c3d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.How do you add dropout to a neural network model to prevent overfitting?\n",
    "Use the Dropout layer in Keras to randomly drop a fraction of neurons during training:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.5),  # Drops 50% of neurons\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6812c-1e14-4c5c-b16f-8ee07b480e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.How do you manually implement forward propagation in a simple neural network?\n",
    "\n",
    "Forward propagation involves matrix multiplication and activation function application. Example in NumPy:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def forward_propagation(X, W, b):\n",
    "    Z = np.dot(X, W) + b  # Linear computation\n",
    "    A = 1 / (1 + np.exp(-Z))  # Sigmoid activation\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cab5e5-261d-4cc2-841a-fffc107a878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.How do you add batch normalization to a neural network model in Keras?\n",
    "\n",
    "Use the BatchNormalization layer in Keras:\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu'),\n",
    "    BatchNormalization(),  # Normalize activations\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e57aec-cc74-41a5-bb27-c6cf085fa4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.How can you visualize the training process with accuracy and loss curves?\n",
    "\n",
    "Use Matplotlib to plot accuracy and loss from the training history:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378202c2-51a4-4264-b4ac-35ec03bee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "9.How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?\n",
    "\n",
    "Specify gradient clipping in the optimizer:\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  # Clip gradients to a maximum norm of 1.0\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de6fee-a3fd-47f9-96ed-f7d4bf338321",
   "metadata": {},
   "outputs": [],
   "source": [
    "10.How can you create a custom loss function in Keras?\n",
    "\n",
    "Define a Python function and pass it as the loss parameter during compilation:\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))  # Mean squared error\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a712bdc-8c5a-4c10-b477-e51a30db8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "11.How can you visualize the structure of a neural network model in Keras?\n",
    "\n",
    "Use the plot_model function from keras.utils:\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
