{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fe5760-250b-44e0-9279-ec473cb1e996",
   "metadata": {},
   "source": [
    "                                    RCNN & Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b35b55-d58f-46a5-9b60-3d0d304d7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the main purpose of RCNN in object detection?\n",
    "\n",
    "RCNN (Region-Based Convolutional Neural Networks) aims to detect objects\n",
    "in an image by generating region proposals, classifying each region, \n",
    "and refining their bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d22e92-a4e2-461e-a05a-7cb77983512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is the difference between Fast RCNN and Faster RCNN?\n",
    "\n",
    "Fast RCNN: Improves RCNN by sharing convolutional layers for the entire image\n",
    "and using a single-stage classifier for region proposals.\n",
    "Faster RCNN: Introduces a Region Proposal Network (RPN) to replace selective search,\n",
    "making region proposal generation faster and more integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc388a-a2b9-4427-bdf2-62b1a5f757e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How does YOLO handle object detection in real-time?\n",
    "\n",
    "YOLO (You Only Look Once) divides the image into a grid, \n",
    "predicting bounding boxes and class probabilities directly from the\n",
    "grid cells using a single forward pass, enabling real-time processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae29d5-6391-40f9-bf5b-78777ac5f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain the concept of Region Proposal Networks (RPN) in Faster RCNN.\n",
    "    \n",
    "RPN generates potential object regions by sliding a small network over the\n",
    "feature map, predicting objectness scores and bounding box offsets, making\n",
    "the process faster and more accurate than selective search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d19716-1db3-40e5-a1f9-31e941e299b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. How does YOLOV5 improve upon its predecessors?\n",
    "\n",
    "YOLOV5 offers faster training, better model scaling,\n",
    "and optimized architectures. It includes improvements \n",
    "like auto-learning anchor boxes, a lighter model, and\n",
    "integration with PyTorch for easier deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efccccdc-ea1a-4601-82e2-27fabce8fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What role does non-max suppression play in YOLO object detection?\n",
    "\n",
    "Non-max suppression (NMS) eliminates redundant bounding boxes by keeping\n",
    "only the box with the highest confidence score, ensuring a single detection per object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581fd01-46d5-4a19-adb9-66db43b21867",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Describe the data preparation process for training YOLOV5.\n",
    "    \n",
    "Annotating images with bounding boxes and class labels.\n",
    "Converting annotations to YOLO format (text files with normalized coordinates).\n",
    "Splitting the dataset into training, validation, and test sets.\n",
    "Applying data augmentation like scaling, rotation, and flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe9a7d-36ec-4f5a-87be-fa87f1358ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What is the significance of anchor boxes in object detection models like YOLOV9?\n",
    "\n",
    "Anchor boxes act as predefined shapes and sizes for bounding box predictions,\n",
    "helping the model better detect objects of varying scales and aspect ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc0e01-12ae-4da9-8f4d-4c3446f988a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. What is the key difference between YOLO and R-CNN architectures?\n",
    "\n",
    "YOLO processes the entire image in a single forward pass, making it faster.\n",
    "R-CNN processes region proposals independently, focusing on accuracy over speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96134892-8ee8-4e64-b1a6-0c9dd110d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Why is Faster RCNN considered faster than Fast RCNN?\n",
    "\n",
    "Faster RCNN uses RPN instead of selective search for \n",
    "generating region proposals, significantly reducing computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f46a9b-1b6a-4c6a-8c13-c31e6939a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. What is the role of selective search in RCNN?\n",
    "\n",
    "Selective search generates region proposals by combining\n",
    "similar pixels based on color, texture, and shape, which\n",
    "are then classified for object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af34e6-582b-454a-8195-2becaa4ea027",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. How does YOLOV5 handle multiple classes in object detection?\n",
    "\n",
    "YOLOV5 predicts probabilities for each class per grid cell,\n",
    "allowing multiple classes to be detected simultaneously within an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514747f-e4dd-4035-a266-4dc8fe8d40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "13. What are the key differences between YOLOV3 and YOLOV9?\n",
    "\n",
    "YOLOV9 introduces advanced features like adaptive computation,\n",
    "transformer-based backbones, and improved scaling techniques,\n",
    "offering higher accuracy and speed compared to YOLOV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6bcb07-913d-4fc7-965b-9b36e819bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. How is the loss function calculated in Faster RCNN?\n",
    "\n",
    "The loss function includes two components:\n",
    "\n",
    "Classification loss: Cross-entropy loss for predicting object classes.\n",
    "Bounding box regression loss: Smooth L1 loss for refining box coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9b69f-0631-46c0-a6cd-580eb21d59a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "15. Explain how YOLOV5 improves speed compared to earlier versions.\n",
    "    \n",
    "YOLOV5 employs lightweight architectures, optimized training pipelines,\n",
    "and hardware acceleration support, significantly boosting inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16dde6-ca3f-4cb7-91ce-3c1a49ec2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "16. What are some challenges faced in training YOLOV5?\n",
    "\n",
    "Insufficient labeled data.\n",
    "Imbalanced datasets.\n",
    "Detecting small or overlapping objects.\n",
    "High computational resource requirements for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5fd2c-52dd-41bc-a441-c13b247ae27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "17. How does the YOLOV5 architecture handle large and small object detection?\n",
    "\n",
    "YOLOV5 uses a multi-scale feature pyramid network (FPN) to \n",
    "detect objects of various sizes effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b2e11-4cec-41fa-87aa-8516cbb3121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "18. What is the significance of fine-tuning in YOLO?\n",
    "\n",
    "Fine-tuning adjusts pre-trained weights on a specific dataset,\n",
    "improving performance for domain-specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132df734-0fcb-4320-a35f-b28549926406",
   "metadata": {},
   "outputs": [],
   "source": [
    "19. What is the concept of bounding box regression in Faster RCNN?\n",
    "\n",
    "Bounding box regression predicts adjustments to initial region \n",
    "proposals to align them more closely with ground truth boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85df5f-06fd-4704-87d1-e181d334cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "20. Describe how transfer learning is used in YOLO.\n",
    "\n",
    "Transfer learning leverages pre-trained models on large datasets,\n",
    "fine-tuning them on smaller, task-specific datasets for better accuracy with fewer resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31601bd-4a5a-427a-9d42-8c1a21916c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "21. What is the role of the backbone network in object detection models like YOLOV5?\n",
    "\n",
    "The backbone extracts essential features from the input image,\n",
    "which are passed to the detection head for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63146e1c-4d1d-4878-a58e-d81322d3ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "22. How does YOLO handle overlapping objects?\n",
    "\n",
    "YOLO uses NMS to resolve overlapping predictions,\n",
    "retaining the box with the highest confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f0f00-7a08-4174-9135-623adb40467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "23. What is the importance of data augmentation in object detection?\n",
    "\n",
    "Data augmentation increases the diversity of training samples,\n",
    "reducing overfitting and improving generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a308845-4e4c-410c-bacc-a7d013f33cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "24. How is performance evaluated in YOLO-based object detection?\n",
    "\n",
    "Metrics include precision, recall, mean Average Precision (mAP),\n",
    "and inference speed (frames per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c056d-f6c0-49fc-92af-8f7cc510222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "25. How do the computational requirements of Faster RCNN compare to those of YOLO?\n",
    "\n",
    "Faster RCNN is more computationally intensive due to its two-stage approach,\n",
    "while YOLO's single-stage approach is more resource-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eddcb3-82b5-4eb0-8a21-8ac94c27d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "26. What role do convolutional layers play in object detection with RCNN?\n",
    "\n",
    "Convolutional layers extract spatial features from images,\n",
    "critical for identifying object boundaries and classifying regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873a575-970f-468d-a3f1-d5285e79d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "27. How does the loss function in YOLO differ from other object detection models?\n",
    "\n",
    "YOLO's loss function combines classification, bounding box regression, \n",
    "and objectness loss into a unified metric, optimized for real-time performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4016862-2b91-4c1a-b8b8-774e8c75992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "28. What are the key advantages of using YOLO for real-time object detection?\n",
    "                                             \n",
    "High speed.\n",
    "Single-stage processing.\n",
    "End-to-end optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9afc3a-9053-48fc-8bd9-994bfccbf317",
   "metadata": {},
   "outputs": [],
   "source": [
    "29. How does Faster RCNN handle the trade-off between accuracy and speed?\n",
    "\n",
    "Faster RCNN uses RPN to balance speed and accuracy, \n",
    "focusing on high-quality region proposals without \n",
    "sacrificing significant computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10bfb05-06fb-4b37-92f2-1a6d553b73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "30. What is the role of the backbone network in both YOLO and Faster RCNN, and how do they differ?\n",
    "\n",
    "YOLO: Uses lightweight backbones like CSPDarkNet for speed.\n",
    "Faster RCNN: Uses more complex backbones (e.g., ResNet) for accuracy,\n",
    "prioritizing feature richness over speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272f4d4-40ea-4361-9dd4-02033d8ad61c",
   "metadata": {},
   "source": [
    "                                   Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc48662-8a1b-4812-b671-55ea80b8f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. How do you load and run inference on a custom image using the YOLOv8 model (labeled as YOLOV9)?\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 (YOLOV9) model\n",
    "model = YOLO(\"yolov8n.pt\")  # Replace 'yolov8n.pt' with the custom-trained model path if needed\n",
    "\n",
    "# Load the custom image\n",
    "image_path = \"path_to_custom_image.jpg\"\n",
    "results = model(image_path)\n",
    "\n",
    "# Display results\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42289609-3665-4f89-9b34-70cdf1bd84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. How do you load the Faster RCNN model with a ResNet50 backbone and print its architecture?\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# Load the Faster RCNN model with ResNet50 backbone\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25df6c-c2c7-47bb-9d00-0141376d3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How do you perform inference on an online image using the Faster RCNN model and print the predictions?\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "# Load the Faster RCNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load the online image\n",
    "url = \"https://example.com/image.jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Transform the image\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "img_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(img_tensor)\n",
    "\n",
    "# Print predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98338f22-7f50-4fe8-ba9d-fd3d5c3b5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. How do you load an image and perform inference using YOLOV9, then display the detected objects with bounding boxes and class labels?\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 (YOLOV9) model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Perform inference\n",
    "image_path = \"path_to_image.jpg\"\n",
    "results = model(image_path)\n",
    "\n",
    "# Annotate the image with bounding boxes and labels\n",
    "annotated_image = results[0].plot()\n",
    "\n",
    "# Display the annotated image\n",
    "cv2.imshow(\"Detected Objects\", annotated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfc076-d64d-4c02-9ef4-4e39ecd3eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. How do you display bounding boxes for the detected objects in an image using Faster RCNN?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "\n",
    "# Load the image and Faster RCNN model\n",
    "image_path = \"path_to_image.jpg\"\n",
    "img = Image.open(image_path)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the image\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "img_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(img_tensor)\n",
    "\n",
    "# Plot the image with bounding boxes\n",
    "plt.imshow(img)\n",
    "ax = plt.gca()\n",
    "for box in predictions[0]['boxes']:\n",
    "    x1, y1, x2, y2 = box.numpy()\n",
    "    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color=\"red\")\n",
    "    ax.add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feefcb4-1024-40f0-a878-0356dcdc2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. How do you perform inference on a local image using Faster RCNN?\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# Load Faster RCNN\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"path_to_image.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image_tensor = F.to_tensor(image).unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor)\n",
    "\n",
    "# Print outputs\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199d61e-1cc8-4d2b-bf05-f26f3228e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. How can you change the confidence threshold for YOLO object detection and filter out low-confidence predictions?\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 (YOLOV9) model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Perform inference with a custom confidence threshold\n",
    "image_path = \"path_to_image.jpg\"\n",
    "results = model(image_path, conf=0.5)  # Set the confidence threshold to 0.5\n",
    "\n",
    "# Display results\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43692a26-b72d-4d3f-8894-a6cd7d126439",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. How do you plot the training and validation loss curves for model evaluation?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example loss data (replace with actual values from your training log)\n",
    "train_loss = [0.9, 0.7, 0.5, 0.4]\n",
    "val_loss = [1.0, 0.8, 0.6, 0.5]\n",
    "\n",
    "# Plot loss curves\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1055e-bfe4-4e41-aa7e-005ba37d2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. How do you perform inference on multiple images from a local folder using Faster RCNN and display the bounding boxes for each?\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "\n",
    "# Load Faster RCNN\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Folder containing images\n",
    "image_folder = \"path_to_folder\"\n",
    "\n",
    "# Loop through images\n",
    "for image_file in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Preprocess and perform inference\n",
    "    img_tensor = T.ToTensor()(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_tensor)\n",
    "\n",
    "    # Print or visualize results\n",
    "    print(f\"Results for {image_file}: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4bef6-9245-4fce-8b24-5fb1a61f4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. How do you visualize the confidence scores alongside the bounding boxes for detected objects using Faster RCNN?\n",
    "\n",
    "# Use the code in (5) and add this line inside the bounding box loop:\n",
    "score = predictions[0]['scores'][i].item()\n",
    "ax.text(x1, y1, f\"{score:.2f}\", color=\"blue\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd12d70-4b9c-45f4-9e93-44f9670ed416",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. How can you save the inference results (with bounding boxes) as a new image after performing detection using YOLO?\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLO\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Perform inference\n",
    "image_path = \"path_to_image.jpg\"\n",
    "results = model(image_path)\n",
    "\n",
    "# Save results\n",
    "results.save(save_dir=\"output_folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
